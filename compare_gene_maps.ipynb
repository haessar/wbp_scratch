{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "gene_maps_dir = \"data/gene_maps\"\n",
    "\n",
    "wbps_map = os.path.join(gene_maps_dir, \"previous_to_current_id.tsv\")\n",
    "lo_map = os.path.join(gene_maps_dir, \"gene_map.few_more.cc_uniq.tsv\")\n",
    "\n",
    "cols = (\"v18\", \"v19\", \"qual\")\n",
    "wbps_df = pd.read_csv(wbps_map, delimiter=\"\\t\", names=cols, index_col=0)\n",
    "wbps_df[\"source\"] = \"wbps\"\n",
    "lo_df = pd.read_csv(lo_map, delimiter=\"\\t\", names=cols, index_col=0)\n",
    "lo_df[\"source\"] = \"lo\"\n",
    "all_df = pd.concat([lo_df, wbps_df]).sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_shared_and_unique(left_df, right_df):\n",
    "    shared_df = pd.DataFrame(index=left_df.index, columns=left_df.columns)\n",
    "    uniq_df = pd.DataFrame(index=left_df.index, columns=left_df.columns)\n",
    "    for idx, row in left_df.iterrows():\n",
    "        shared = False\n",
    "        if idx in right_df.index:\n",
    "            for i2, r2 in right_df[right_df.index==idx].iterrows():\n",
    "                if r2.v19 == row.v19:\n",
    "                    shared = True\n",
    "        if shared:\n",
    "            shared_df = pd.concat([shared_df, pd.DataFrame([row])])\n",
    "        else:\n",
    "            uniq_df = pd.concat([uniq_df, pd.DataFrame([row])])\n",
    "    return shared_df.dropna(), uniq_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4142/514161461.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  shared_df = pd.concat([shared_df, pd.DataFrame([row])])\n",
      "/tmp/ipykernel_4142/514161461.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  uniq_df = pd.concat([uniq_df, pd.DataFrame([row])])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mappings shared: 8345\n"
     ]
    }
   ],
   "source": [
    "shared_df, _ = split_shared_and_unique(wbps_df, lo_df)\n",
    "print(f\"mappings shared: {len(shared_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine unique mappings from forward and reverse Liftoff runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lo_for_map = os.path.join(gene_maps_dir, \"gene_map_1.cc.tsv\")\n",
    "lo_for_df = pd.read_csv(lo_for_map, delimiter=\"\\t\", names=cols, index_col=0)\n",
    "lo_rev_map = os.path.join(gene_maps_dir, \"gene_map_0.cc.tsv\")\n",
    "lo_rev_df = pd.read_csv(lo_rev_map, delimiter=\"\\t\", names=cols, index_col=0)\n",
    "\n",
    "_, uniq_rev = split_shared_and_unique(lo_rev_df, lo_for_df)\n",
    "_, uniq_for = split_shared_and_unique(lo_for_df, lo_rev_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique mappings from forward Liftoff run (18 -> 19): 70\n",
      "...not in WBPS mapping: 56\n",
      "Unique mappings from reverse Liftoff run (19 -> 18): 76\n",
      "...not in WBPS mapping: 53\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique mappings from forward Liftoff run (18 -> 19): {len(uniq_for)}\")\n",
    "print(f\"...not in WBPS mapping: {len(uniq_for[~uniq_for.index.isin(wbps_df.index)])}\")\n",
    "print(f\"Unique mappings from reverse Liftoff run (19 -> 18): {len(uniq_rev)}\")\n",
    "print(f\"...not in WBPS mapping: {len(uniq_rev[~uniq_rev.index.isin(wbps_df.index)])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine how many v19 genes are missed by both mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gffutils\n",
    "import os.path\n",
    "\n",
    "v19_path = \"data/from_WBPS/strongyloides_stercoralis.PRJNA930454.WBPS19.annotations.gff3\"\n",
    "\n",
    "if not os.path.exists(\"db/v19.db\"):\n",
    "    db = gffutils.create_db(v19_path, \"db/v19.db\", merge_strategy=\"create_unique\")\n",
    "else:\n",
    "    db = gffutils.FeatureDB(\"db/v19.db\")\n",
    "\n",
    "missing_v19_lo = set()\n",
    "missing_v19_wbps = set()\n",
    "for g in db.all_features(featuretype=\"gene\"):\n",
    "    g_id = g.id.split(\":\")[1]\n",
    "    if lo_df[lo_df.v19 == g_id].empty:\n",
    "        missing_v19_lo.add(g_id)\n",
    "    if wbps_df[wbps_df.v19 == g_id].empty:\n",
    "        missing_v19_wbps.add(g_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total WBPS19 genes: 12061\n",
      "Missing from Liftoff mappings: 3141\n",
      "Missing from WBPS mappings: 2279\n",
      "Missing from both: 1953\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total WBPS19 genes: {len(list(db.all_features(featuretype='gene')))}\")\n",
    "print(f\"Missing from Liftoff mappings: {len(missing_v19_lo)}\")\n",
    "print(f\"Missing from WBPS mappings: {len(missing_v19_wbps)}\")\n",
    "print(f\"Missing from both: {len(missing_v19_lo.intersection(missing_v19_wbps))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(gene_maps_dir, \"missing_v19_ids.txt\"), \"w\") as f:\n",
    "    for g_id in sorted(missing_v19_lo.intersection(missing_v19_wbps)):\n",
    "        f.write(f\"{g_id}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine how many v18 genes are missed by both mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gffutils\n",
    "\n",
    "v18_path = \"data/from_WBPS/strongyloides_stercoralis.PRJEB528.WBPS18.annotations.gff3\"\n",
    "\n",
    "if not os.path.exists(\"db/v18.db\"):\n",
    "    db18 = gffutils.create_db(v18_path, \"db/v18.db\", merge_strategy=\"create_unique\")\n",
    "else:\n",
    "    db18 = gffutils.FeatureDB(\"db/v18.db\")\n",
    "\n",
    "missing_v18_lo = set()\n",
    "missing_v18_wbps = set()\n",
    "for g in db18.all_features(featuretype=\"gene\"):\n",
    "    g_id = g.id.split(\":\")[1]\n",
    "    if lo_df[lo_df.index == g_id].empty:\n",
    "        missing_v18_lo.add(g_id)\n",
    "    if wbps_df[wbps_df.index == g_id].empty:\n",
    "        missing_v18_wbps.add(g_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total WBPS18 genes: 13123\n",
      "Missing from Liftoff mappings: 2895\n",
      "Missing from WBPS mappings: 3341\n",
      "Missing from both: 1486\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total WBPS18 genes: {len(list(db18.all_features(featuretype='gene')))}\")\n",
    "print(f\"Missing from Liftoff mappings: {len(missing_v18_lo)}\")\n",
    "print(f\"Missing from WBPS mappings: {len(missing_v18_wbps)}\")\n",
    "print(f\"Missing from both: {len(missing_v18_lo.intersection(missing_v18_wbps))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(gene_maps_dir, \"missing_v18_ids.txt\"), \"w\") as f:\n",
    "    for g_id in sorted(missing_v18_lo.intersection(missing_v18_wbps)):\n",
    "        f.write(f\"{g_id}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran missing_v18_ids.txt through a WBPS BioMart query to gather Pfam domains associated with them, which I've saved in data/from_WBPS/sstp_pfams.txt. From the paper https://www.nature.com/articles/s41598-019-40965-0#Sec2 and the book https://link.springer.com/protocol/10.1007/978-1-4939-9173-0_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total \"lost\" WBPS18 genes: 1486\n",
      "....that have no Pfam domain: 709\n",
      "....that have at least 1 Pfam domain: 739\n",
      "........any of which are associated with transposable elements: 125\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/transposable_elements/transposable_element_pfams.json\") as f:\n",
    "    te_pfams = json.loads(f.read())\n",
    "\n",
    "v18_pfams = pd.read_csv(\"data/from_WBPS/sstp_pfams.txt\", delimiter=\"\\t\")\n",
    "te_genes = set()\n",
    "\n",
    "print(f\"Total \\\"lost\\\" WBPS18 genes: {len(missing_v18_lo.intersection(missing_v18_wbps))}\")\n",
    "print(f\"....that have no Pfam domain: {len(v18_pfams[v18_pfams.ID.isna()])}\")\n",
    "print(f\"....that have at least 1 Pfam domain: {len(v18_pfams[~v18_pfams.ID.isna()]['Gene stable ID'].unique())}\")\n",
    "for idx, row in v18_pfams[~v18_pfams.ID.isna()].iterrows():\n",
    "    if row[\"ID\"] in te_pfams:\n",
    "        te_genes.add(row[\"Gene stable ID\"])\n",
    "print(f\"........any of which are associated with transposable elements: {len(te_genes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(gene_maps_dir, \"missing_v18_ids_TEs.txt\"), \"w\") as f:\n",
    "    for g_id in sorted(te_genes):\n",
    "        f.write(f\"{g_id}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
